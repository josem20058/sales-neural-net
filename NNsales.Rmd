---
title: "neualNetSales"
author: "Jose Muralles"
date: "11/10/2019"
output: 
  html_document:
    toc: true
    number_sections: true
    theme: united
---
# Introduction
This is an application of a neural network designed to identify characterists of car buyers.  The purose is to direct advertising.

**ASSIGNMENT:**    
  1. Find the 'best' sales target price for predicting sales -  
  2. Determing values of age, gender, income level or commute distance *i.e.* miles/wk that predict best sales  
  3. Determing the appropriate neural net - how many hidden layers, what variables to leave out?  
  4. Does it help to use 80/20 (or any other) split for training/test sets?  
  5. Recommend an advetising approach based on your findings  
  
## first set up the environment

<!-- Assumes the .csv file 'cars.csv' is in the working directory !-->  

```{r setup, include=FALSE}
set.seed(500)
knitr::opts_chunk$set(echo = TRUE)
library(neuralnet)
cars = read.csv('cars.csv', header=TRUE)
```

## table ..  description of the data
The data set contains information about car buyers

Variable       |  Description
---------------|----------------------------------------
age            | age of the buyer
gender         | sex of the buyer
miles          | average number of miles driven per day
debt           | current debt of the buyer
income         | buyer montly income
sales          | amount spent on a used car




## print the first 6 entries of the cars data set

=====  
set the target price for sales  
=====  

```{r}
salesTargetPrice = 11000
summary(cars$sales)
hist(cars$sales, xlab='price', main='Used Car Sales Prices')
boxplot(cars$sales, main='sales prices',horizontal = TRUE, xlab='$')
grid()
# set up 'yes / no' based on sales price salesTargetPrice
cars$salesTgt = ifelse(cars$sales>salesTargetPrice,1,0)
head(cars)
```

# Set up the Training and Test data  

=====    
set the training fraction  
=====


```{r}
##set up the training and test data:
trainingFraction = 0.75
lengthData = length(cars$age) #  how many entries in the data set
nTrain = round(trainingFraction*lengthData,0)
nTest = lengthData-nTrain
# shuffle the data, take the first nTrain as the training set, the 
# remainder as the test set
shuffle = sample(1:lengthData, replace=FALSE) 
#set up the test and train objects
testDataSet = vector("numeric", length=nTest)
trainDataset= vector("numeric", length=nTrain)
trainIndex = shuffle[1:nTrain]
trainDataSet = cars[shuffle[1:nTrain],]
testIndex = vector("numeric")
testIndex = shuffle[-trainIndex]
testDataSet = cars[shuffle[-trainIndex],]
head(trainDataSet)
```

Set up normalized set-- based on the test set

```{r}
meanAge = mean(testDataSet$age)
sdAge = sd(testDataSet$age)
meanMiles = mean(testDataSet$miles)
sdMiles = sd(testDataSet$miles)
meanDebt = mean(testDataSet$debt)
sdDebt = sd(testDataSet$debt)
meanIncome = mean(testDataSet$income)
sdIncome = sd(testDataSet$income)
#normalize age, miles, debt, income, and sales
## normalize the entire data set based on the test set..
carsNormalized = cars
carsNormalized$age = (cars$age-meanAge)/sdAge
carsNormalized$miles = (cars$miles-meanMiles)/sdMiles
carsNormalized$debt = (cars$debt - meanDebt)/sdDebt
carsNormalized$income = (cars$income-meanIncome)/sdIncome
carsNormalized$salesTgt = cars$salesTgt
normalizedTrainDataSet= carsNormalized[shuffle[trainIndex],]
normalizedTestDataSet = carsNormalized[shuffle[-trainIndex],]
head(normalizedTrainDataSet)
```


# what factors may be in play??
  
```{r}
linearModel = glm(sales~ age + gender + miles + debt + income, data = trainDataSet)
summary(linearModel)
print(linearModel)
```

 leave gender out  
 
# Traditional stat approach: Logistic Regression  

```{r}
logisticModel = glm(salesTgt ~ age + miles + debt + income, family=binomial, data = trainDataSet)
summary(logisticModel)
b = as.numeric(logisticModel$coefficients)
```

## quick look- general probability of a sale  
print probabilities of sale based on age from logistic regression

```{r}
# use average values for debt, income, and miles driven/wk
averageDebt = mean(testDataSet$debt)
averageIncome = mean(testDataSet$income)
averageMiles = mean(testDataSet$miles)
# logistic regresson coefficients from previous chunk
xx = b[1]+b[2]*testDataSet$age + b[3]*averageMiles +
  b[4]*averageDebt + b[5]*averageIncome
pred.prob = 1./(1+exp(xx))
plot(testDataSet$age, pred.prob, ylab='prob of sale', xlab='age')
grid()
```

## Cross tabulation of logistic regression predictions  
columns are 'truth,' rows are predictions

```{r echo=FALSE}
# set up the table column
xTabLogis = matrix(0,nrow=2, ncol=2)
probSale = 0.2
for(i in 1:nTest){
  
  if((pred.prob[i]>probSale) && (testDataSet$salesTgt[i]==1) )
     xTabLogis[2,2] = xTabLogis[2,2]+1
     
  if((pred.prob[i]>probSale) && (testDataSet$salesTgt[i]==0))
     xTabLogis[2,1] = xTabLogis[2,1]+1
  if((pred.prob[i]<=probSale) && (testDataSet$salesTgt[i]==0))
     xTabLogis[1,1] = xTabLogis[1,1]+1
  if((pred.prob[i]<=probSale) && (testDataSet$salesTgt[i]==1))
     xTabLogis[1,2] = xTabLogis[1,2]+1
      
}
print(xTabLogis)
print('columns are truth in data set, ')
print('rows are predicted by logistic regression ')
accuracyLogis = (xTabLogis[1,1]+xTabLogis[2,2])/nTest
print(paste('Overall accuracy: ', round(accuracyLogis,2)*100,'%', sep=''))
```


# On with the NN  

## Use the normalized data!  
set up the nerual net  

```{r SalesNet}
# set up the model using the training set
salesNet = neuralnet(salesTgt~age+miles+debt+income, data = normalizedTrainDataSet, hidden=c(3), stepmax=1.e6, rep=1, linear.output = FALSE)
```

plot..

```{r  fig.keep='all', fig.width=100}
plot(salesNet)
```

## cross tabulation for the NN

NN cross tabulation; columns are 'truth', rows are predictions

```{r echo=FALSE}
xTabNN = matrix(0,nrow=2, ncol=2)
for(i in 1:nTest){
  
  if((salesNet$response[i]==1) && (testDataSet$salesTgt[i]==1) )
     xTabNN[2,2] = xTabNN[2,2]+1
     
  if((salesNet$response[i]==1) && (testDataSet$salesTgt[i]==0))
     xTabNN[2,1] = xTabNN[2,1]+1
  if((salesNet$response[i]==0) && (testDataSet$salesTgt[i]==0))
     xTabNN[1,1] = xTabNN[1,1]+1
  if((salesNet$response[i]==0) && (testDataSet$salesTgt[i]==1))
     xTabNN[1,2] = xTabNN[1,2]+1
      
}
print(xTabNN)
print('columns are truth in data set, ')
print('rows are predicted by logistic regression ')
accuracyNN = (xTabNN[1,1]+xTabNN[2,2])/nTest
print(paste('Overall accuracy: ', round(accuracyNN,3)))
```

## look at a few of the fit values..

```{r echo=FALSE}
randomTest = sample(1:length(normalizedTestDataSet$salesTgt), 15)
x = data.frame(age=normalizedTestDataSet$age[randomTest],     miles=normalizedTestDataSet$miles[randomTest],
    debt= normalizedTestDataSet$debt[randomTest], 
    income= normalizedTestDataSet$income[randomTest])
print(x)
print('Actual    Predicted')
for (i in 1:15)
  print(paste(normalizedTestDataSet$salesTgt[randomTest][i],'     ',salesNet$response[randomTest][i]))
```

Set up an example subset of expected buyers


=====    
set up x, the input vector based on values you think
are appropriate for advertising  
=====


```{r}
library(dplyr)
# set test conditions
# set up x with conditons on age, income, debt and miles you think 
# would be appropriate for advertising
#use dply function 'filter' to pull appropriate data from test set
x = filter(testDataSet, age< 45 & income>6000 & debt < 12000 & miles > 25)
# to use in the neural net, normalize the test set data
# notice that we use mean and sd values computed from the
# train set.  The deal is, we don't know the mean and sd from 
# future observations..   until we use that data to update the model
for (i in length(x$age)){
  x$age = (x$age-meanAge)/sdAge
  x$miles = (x$miles-meanMiles)/sdMiles
  x$debt = (x$debt - meanDebt)/sdDebt
  x$income = (x$income-meanIncome)/sdIncome
}
# print the values of the normalized inputs
print(x)
# make predicitions... 
predX = predict(salesNet, x)
# print the results predicted by the neural net
table(predX[,1]>0.5, x$salesTgt)

```

 0 and 1 are truth from the train sets,   
 TRUE -- predicted  '1'  
 FALSE -- predicted  '0'  
 
 **PROJECT RESULTS:**    
  1. Find the 'best' sales target price for predicting sales -  
  I wanted to get the results to reach above 50 percent. I know that the car price should be around $9130 because that is what the median car price is. I tried different prices around 7000 to the 15000 dollar range. My experiments showed me that my best results were when i settled around 11000 as a price range for predicting sales. This is consistent with the logic that moderately priced cars tend to be in good quality, and selling them is easier than selling junk cars and high end vehicles.
  
  2. Determine values of age, gender, income level or commute distance *i.e.* miles/wk that predict best sales  
  Gender was not very hepful in predicting sales so i removed it from the test set. Age was a useful category It increased the proportion of accurate predictions. As suspected Income was a strong predictor of the person's capacity to buy a car in the 11000 dollar range. I narrowed the focus to people who had an income higher than 6000, which is slightly bellow the median value of income from the sample data.Debt is access to money, so it was a strong predictor of whether or not a buyer has a credit history. I obtained good recults when i messed with the debt value. It seems there is a trend when people accumulate debt histoircaly they are more likely to continue to do so.
The avaerage income was about 6000 dollars. 12000 dollars is roughly equal to two years of income, and more debt than that would mean that a person isn't handling their finances well. I limited the amount of debt 120o about 12000. Most people drive their cars between 25 and 30 miles a day or less. I narrowed the focus of the algorithm to select drivers who drive less than 25 miles a day.
  
  3. Determing the appropriate neural net - how many hidden layers, what variables to leave out?  
I really tried to make sure that my machine could run the neural network. I went with using one layer and about 3 nodes because when i tried other inputs i got error messages and the code did not run properly or give me favorable results. The more similators there are the better the model will be.
In the prediction model I decided to leave out the miles variable, since it seemed less significant than income and debt when i experimented with changing values.
 
 
  4. Does it help to use 80/20 (or any other) split for training/test sets?  
  I used three splits, 66/34 75/25 80/20. there was a significant drop in accuracy when i only used 2/3rds of the data to train the algorithm. There was notable improvement when I used 3/4ths of the data for training, and not much improvement between 75/25 and a 80/20 split. I decided that 75/25 split was a more measured approach than 80/20 but still retained accuracy i levels i was satisfied with. 
  
  5. Recommend an advetising approach based on your findings  campaign company should focus on customers below 45 years old and who earn more than $6,000. It is actually good i they have debt because it refelcts that they have credit activity which is good so long as it is in good staanding.
 This ad targets people who drive less than 25 miles a day, because that input made a small increase in the models prediction rates. 
 
